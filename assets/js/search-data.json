{
  
    
        "post0": {
            "title": "Pytorch Train/Test Using Pretrained Model and Artificial Data",
            "content": "Imports . import torch, torchvision, torch.optim as optim . Create Functions . def get_device(): &quot;&quot;&quot;Set as GPU if available, else set CPU&quot;&quot;&quot; if torch.cuda.is_available(): return torch.device(&#39;cuda&#39;) else: return torch.device(&#39;cpu&#39;) class one_zero_dataset(torch.utils.data.Dataset): &quot;&quot;&quot;Setup zero/one gen as a dataset&quot;&quot;&quot; def __len__(self): return 100 #Set dataset size to be 100 def __getitem__(self, index): #Create tensors of the same shape as Imagenet with either a 1 or 0 if (index%2) == 0: data = torch.zeros((3, 224, 224), dtype=torch.float32) gnd_trth = float(0) else: data = torch.ones((3, 224, 224), dtype=torch.float32) gnd_trth = float(1) return data, gnd_trth def train_single_epoch(device, dataloader, model, criterion, optimizer): #zero accumulating values for los and acc running_acc = 0.0 running_loss = 0.0 #set model to train state model.train() # Iterate over data and train for batch, gnd_truth in dataloader: #Send to gpu or cpu batch, gnd_truth = batch.to(device), gnd_truth.to(device) # zero the parameter gradients optimizer.zero_grad() # forward preds = model(batch) #get loss loss = criterion(preds[:,1], gnd_truth.float()) #backward loss.backward() #optimize optimizer.step() # update running loss and acc running_acc += (torch.round(preds[:,1]).unsqueeze(1) == gnd_truth.unsqueeze(1)).sum()/ gnd_truth.unsqueeze(1).shape[0] running_loss += loss.item() return model, running_loss/len(dataloader), running_acc/len(dataloader) def test_single_epoch(device, dataloader, model, criterion, optimizer): #zero accumulating values for los and acc running_acc = 0.0 running_loss = 0.0 #set model to eval state with torch.no_grad(): # Iterate over data and test for batch, gnd_truth in dataloader: #Send to gpu or cpu batch, gnd_truth = batch.to(device), gnd_truth.to(device) # forward preds = model(batch) #get loss loss = criterion(preds[:,1], gnd_truth.float()) # update running loss and acc running_acc += (torch.round(preds[:,1]).unsqueeze(1) == gnd_truth.unsqueeze(1)).sum()/ gnd_truth.unsqueeze(1).shape[0] running_loss += loss.item() return running_loss/len(dataloader), running_acc/len(dataloader) . Setting Hyper Parameters . device = get_device() learning_rate = 0.0001 batch_size = 10 epochs = 10 . Import and Setup Model . model = torchvision.models.resnet18(pretrained=True) model.fc = torch.nn.Sequential(torch.nn.Linear(in_features=512, out_features=2, bias=True), torch.nn.Softmax(dim = 1)) model = model.to(device) . Setup Optimizer and Criterion . optimizer = optim.Adam(model.parameters(), lr=learning_rate) criterion = torch.nn.BCEWithLogitsLoss() . Setup DataLoaders . train_data_loader = torch.utils.data.DataLoader(one_zero_dataset(), batch_size=batch_size, shuffle=True, num_workers=12) test_data_loader = torch.utils.data.DataLoader(one_zero_dataset(), batch_size=batch_size, shuffle=True, num_workers=2) . Train and Test Model . print(&#39;&#39;) print(&#39;starting training for {} epochs &#39;.format(epochs)) # loop over the dataset multiple times training and then testing for epoch in range(1, epochs+1): model, train_loss, train_acc = train_single_epoch(device, train_data_loader, model, criterion, optimizer) test_loss, test_acc = test_single_epoch(device, test_data_loader, model, criterion, optimizer) if (epoch % 2) == 0: #Print every 2 epochs print(&#39;finished training for epoch {}&#39;.format(epoch)) print(&#39;train_loss: {:.4f}&#39;.format(train_loss)) print(&#39;test_loss: {:.4f}&#39;.format(test_loss)) print(&#39;train_acc: {:.4f}&#39;.format(train_acc)) print(&#39;test_acc: {:.4f}&#39;.format(test_acc)) print(&#39; n- n&#39;) . starting training for 10 epochs finished training for epoch 2 train_loss: 0.5039 test_loss: 0.5037 train_acc: 1.0000 test_acc: 1.0000 - finished training for epoch 4 train_loss: 0.5033 test_loss: 0.5037 train_acc: 1.0000 test_acc: 1.0000 - finished training for epoch 6 train_loss: 0.5077 test_loss: 0.5039 train_acc: 1.0000 test_acc: 1.0000 - finished training for epoch 8 train_loss: 0.5035 test_loss: 0.5034 train_acc: 1.0000 test_acc: 1.0000 - finished training for epoch 10 train_loss: 0.5033 test_loss: 0.5036 train_acc: 1.0000 test_acc: 1.0000 - .",
            "url": "https://kierenaw.github.io/blog/2021/08/04/pytorch_artificial_data_demo.html",
            "relUrl": "/2021/08/04/pytorch_artificial_data_demo.html",
            "date": " • Aug 4, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://kierenaw.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://kierenaw.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}