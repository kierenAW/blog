{
  
    
        "post0": {
            "title": "Changing Index into One-Hot Encoding with Pytorch",
            "content": "In this post we cover how to use torch.nn.functional to encode a integer label (e.g. 4 or 0 or 6) into a one-hot encoded tensor (e.g. [0, 0, 0, 1, 0, 0] or [1, 0, 0, 0, 0] or [0, 0, 0, 0, 1]) . Why Use One-Hot Encoder . In many classification tasks the use of one-hot encoding of labels is needed by some loss functions, this is especially true when there is more than 2 classes. . The benefit of one-hot encoding is that the neural network can have an output neuron for each class, when a Softmax is applied just before the output results in an output tensor where all values add up to 1.0 and this is often read as likelihood. . Example . An example could be classes cat, dog, cow and a photo of a cow might result in an output tensor that looks like this [.05, 0.5, .90] therefore we might describe this as having 5% likelihood of being cat or dog and 90% of being a cow. . The ground truth tensor for this case might look like [0, 0, 1] and the tensor of the difference between the predictions and ground truth could be described as [.05, .05, .1] . Reproducing Code . This code was created and tested using Environment E037 . Imports . We will only need torch to be imported for this example . import torch . Example function . We use torch’s random function to create a tensor of the chosen length while randomly setting the class to the selected number indexing from zero. . def one_hot_encode_demo(num_classes = 2, idx_len = 10): print(&#39;Randomly generating index..... n&#39;) idx = torch.Tensor(idx_len).random_(0, num_classes) print(&#39;The index is now&#39;, idx) print(&#39; nCreating one-hot encoded tensor....&#39;) one_hot = torch.nn.functional.one_hot(idx.to(torch.int64), num_classes) print(&#39; nThe one-hot encoded index is now:&#39;) print(one_hot) . Display Some Examples . Now we create a very simple example with 2 classes to demonstrate the basic concept . one_hot_encode_demo(num_classes = 2) . Randomly generating index..... The index is now tensor([0., 0., 0., 1., 1., 0., 0., 1., 0., 0.]) Creating one-hot encoded tensor.... The one-hot encoded index is now: tensor([[1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0]]) . Now we create a more complex example with 10 classes . one_hot_encode_demo(num_classes = 10) . Randomly generating index..... The index is now tensor([7., 8., 2., 5., 9., 0., 6., 1., 5., 4.]) Creating one-hot encoded tensor.... The one-hot encoded index is now: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]]) . As a final example we create a tensor of 3 classes and a length of 25 . one_hot_encode_demo(num_classes = 3, idx_len = 25) . Randomly generating index..... The index is now tensor([2., 1., 1., 2., 1., 2., 2., 1., 0., 2., 1., 2., 1., 2., 1., 0., 2., 0., 2., 2., 0., 0., 1., 0., 0.]) Creating one-hot encoded tensor.... The one-hot encoded index is now: tensor([[0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0], [0, 0, 1], [0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 0, 1], [0, 1, 0], [0, 0, 1], [0, 1, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [0, 0, 1], [0, 0, 1], [1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0]]) .",
            "url": "https://kierenaw.github.io/blog/2021/08/18/using_pytorch_to_convert_from_an_index_into_one-hot_encoding.html",
            "relUrl": "/2021/08/18/using_pytorch_to_convert_from_an_index_into_one-hot_encoding.html",
            "date": " • Aug 18, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Pytorch Train/Test Using Pretrained Model and Artificial Data",
            "content": "This is a demonstration of using Pytorch to fine tune a pre-trained model on an artificial data set (zeros and ones). This code can be a good place to start from when creating a new Neural Network in PyTorch as it demonstrates many of the basic concepts in a functional context. . This model trains very fast due to the extremely simple nature of the task and will run on either CPU or GPU auto selecting GPU if available. . Included are the basic elements: . Selecting GPU/CPU | Building a Pytorch dataset | Setting hyper parameters | Importing a pre-trained model architecture | Setting optimizer and criterion | Creating Pytroch DataLoader | Creating Train/Test functions | Training then Testing using a loop with outputs | This code was created and tested using Enviroment E037 . Imports . import torch, torchvision, torch.optim as optim . Create Functions . def get_device(): &quot;&quot;&quot;Set as GPU if available, else set CPU&quot;&quot;&quot; if torch.cuda.is_available(): return torch.device(&#39;cuda&#39;) else: return torch.device(&#39;cpu&#39;) class one_zero_dataset(torch.utils.data.Dataset): &quot;&quot;&quot;Setup zero/one gen as a dataset&quot;&quot;&quot; def __len__(self): return 100 #Set dataset size to be 100 def __getitem__(self, index): #Create tensors of the same shape as Imagenet with either a 1 or 0 if (index%2) == 0: data = torch.zeros((3, 224, 224), dtype=torch.float32) gnd_trth = float(0) else: data = torch.ones((3, 224, 224), dtype=torch.float32) gnd_trth = float(1) return data, gnd_trth def train_single_epoch(device, dataloader, model, criterion, optimizer): #zero accumulating values for los and acc running_acc = 0.0 running_loss = 0.0 #set model to train state model.train() # Iterate over data and train for batch, gnd_truth in dataloader: #Send to gpu or cpu batch, gnd_truth = batch.to(device), gnd_truth.to(device) # zero the parameter gradients optimizer.zero_grad() # forward preds = model(batch) #get loss loss = criterion(preds[:,1], gnd_truth.float()) #backward loss.backward() #optimize optimizer.step() # update running loss and acc running_acc += (torch.round(preds[:,1]).unsqueeze(1) == gnd_truth.unsqueeze(1)).sum()/ gnd_truth.unsqueeze(1).shape[0] running_loss += loss.item() return model, running_loss/len(dataloader), running_acc/len(dataloader) def test_single_epoch(device, dataloader, model, criterion, optimizer): #zero accumulating values for los and acc running_acc = 0.0 running_loss = 0.0 #set model to eval state with torch.no_grad(): # Iterate over data and test for batch, gnd_truth in dataloader: #Send to gpu or cpu batch, gnd_truth = batch.to(device), gnd_truth.to(device) # forward preds = model(batch) #get loss loss = criterion(preds[:,1], gnd_truth.float()) # update running loss and acc running_acc += (torch.round(preds[:,1]).unsqueeze(1) == gnd_truth.unsqueeze(1)).sum()/ gnd_truth.unsqueeze(1).shape[0] running_loss += loss.item() return running_loss/len(dataloader), running_acc/len(dataloader) . Setting Hyper Parameters . device = get_device() learning_rate = 0.0001 batch_size = 10 epochs = 10 . Import and Setup Model . model = torchvision.models.resnet18(pretrained=True) model.fc = torch.nn.Sequential(torch.nn.Linear(in_features=512, out_features=2, bias=True), torch.nn.Softmax(dim = 1)) model = model.to(device) . Setup Optimizer and Criterion . optimizer = optim.Adam(model.parameters(), lr=learning_rate) criterion = torch.nn.BCEWithLogitsLoss() . Setup DataLoaders . train_data_loader = torch.utils.data.DataLoader(one_zero_dataset(), batch_size=batch_size, shuffle=True, num_workers=12) test_data_loader = torch.utils.data.DataLoader(one_zero_dataset(), batch_size=batch_size, shuffle=True, num_workers=2) . Train and Test Model . print(&#39;&#39;) print(&#39;starting training for {} epochs &#39;.format(epochs)) # loop over the dataset multiple times training and then testing for epoch in range(1, epochs+1): model, train_loss, train_acc = train_single_epoch(device, train_data_loader, model, criterion, optimizer) test_loss, test_acc = test_single_epoch(device, test_data_loader, model, criterion, optimizer) if (epoch % 2) == 0: #Print every 2 epochs print(&#39;finished training for epoch {}&#39;.format(epoch)) print(&#39;train_loss: {:.4f}&#39;.format(train_loss)) print(&#39;test_loss: {:.4f}&#39;.format(test_loss)) print(&#39;train_acc: {:.4f}&#39;.format(train_acc)) print(&#39;test_acc: {:.4f}&#39;.format(test_acc)) print(&#39; n- n&#39;) . starting training for 10 epochs finished training for epoch 2 train_loss: 0.5039 test_loss: 0.5037 train_acc: 1.0000 test_acc: 1.0000 - finished training for epoch 4 train_loss: 0.5033 test_loss: 0.5037 train_acc: 1.0000 test_acc: 1.0000 - finished training for epoch 6 train_loss: 0.5077 test_loss: 0.5039 train_acc: 1.0000 test_acc: 1.0000 - finished training for epoch 8 train_loss: 0.5035 test_loss: 0.5034 train_acc: 1.0000 test_acc: 1.0000 - finished training for epoch 10 train_loss: 0.5033 test_loss: 0.5036 train_acc: 1.0000 test_acc: 1.0000 - .",
            "url": "https://kierenaw.github.io/blog/2021/08/04/pytorch_artificial_data_demo.html",
            "relUrl": "/2021/08/04/pytorch_artificial_data_demo.html",
            "date": " • Aug 4, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://kierenaw.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://kierenaw.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}